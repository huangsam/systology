<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Scalable Model Serving Inference - Systology</title><meta name=description content="Real-time ML inference at scale."><link rel=canonical href=https://sambyte.net/systology/designs/model-serving/><link rel=icon href=/systology/favicon.svg type=image/svg+xml><meta name=robots content="index, follow"><script>try{localStorage.getItem("theme")==="dark"&&document.documentElement.setAttribute("data-theme","dark")}catch{}</script><style>html{background:#fff;color:#0f1724}html[data-theme=dark]{background:#0f1724;color:#e2e8f0}</style><link rel=stylesheet href=/systology/css/styles.css><link rel=stylesheet href=/systology/css/syntax.css><link rel=stylesheet href=/systology/css/search.css></head><body><header class=site-header role=banner><div class="container header-inner"><a class=brand href=/systology/ aria-label=Systology><img src=/systology/favicon.svg alt class=logo-icon> Systology</a><nav class=site-nav role=navigation aria-label="Main navigation"><button class=search-toggle data-open-search aria-label=Search>
<svg width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="11" cy="11" r="8"/><line x1="21" y1="21" x2="16.65" y2="16.65"/></svg>
</button>
<button class=theme-toggle id=theme-toggle aria-label="Toggle dark mode">
<svg class="icon-moon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg class="icon-sun" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></nav></div></header><main id=content class="container prose"><article><header><div class=title-group><h1>Scalable Model Serving Inference</h1></div><h2>Real-time ML inference at scale.</h2><div class=tags><span>Tags:</span>
<a class=badge href=/systology/tags/ml/>ml</a>
<a class=badge href=/systology/tags/monitoring/>monitoring</a></div></header><section><h2 id=problem-statement--constraints>Problem Statement & Constraints</h2><p>Design an infrastructure to serve machine learning models for real-time inference, supporting high throughput and low latency while providing resource-aware fallbacks. The system must ensure deterministic results, handle model versioning, and scale horizontally to accommodate varying loads without compromising availability or performance.</p><h3 id=functional-requirements>Functional Requirements</h3><ul><li>Serve trained ML models for inference requests.</li><li>Support model versioning and canary rollouts.</li><li>Provide fallback models on errors or resource constraints.</li></ul><h3 id=non-functional-requirements>Non-Functional Requirements</h3><ul><li><strong>Scale:</strong> 10k inferences/sec; multi-replica horizontal scaling.</li><li><strong>Availability:</strong> 99.9% inference availability.</li><li><strong>Consistency:</strong> Deterministic results for same input across replicas.</li><li><strong>Latency:</strong> P99 &lt; 200ms end-to-end inference.</li><li><strong>Workload Profile:</strong><ul><li>Read:Write ratio: ~99:1</li><li>Peak throughput: 10k inferences/sec</li><li>Retention: last 10 model versions + 30-day metrics</li></ul></li></ul><h2 id=high-level-architecture>High-Level Architecture</h2><div class=mermaid>graph TD
Client["Client<br>(Inference Request)"]
Gateway["Gateway<br>(Load Balance)"]
Router["Router<br>(Canary Decision)"]
ServerA["Server A<br>(Canary ~5-10%)"]
ServerB["Server B<br>(Stable ~90-95%)"]
GPUA["GPU Pool A"]
GPUB["GPU Pool B"]
Registry["Model Registry<br>(Versions)"]
Fallback["Fallback Model"]
Client --> Gateway
Gateway --> Router
Router -->|canary traffic| ServerA
Router -->|production traffic| ServerB
ServerA --> GPUA
ServerB --> GPUB
ServerA -.->|load| Registry
ServerB -.->|load| Registry
ServerA -.->|on error| Fallback
ServerB -.->|on error| Fallback</div><p>A Gateway forwards client requests to a Router that splits traffic between a small Canary pool and a large Stable production pool. Both pools dynamically execute versioned models from a Registry on dedicated GPUs. Errors or latency spikes automatically reroute to a lightweight Fallback Model to preserve availability.</p><h2 id=data-design>Data Design</h2><p>An immutable Model Registry stores versioned binaries and runtimes, enabling instant rollbacks. Volatile Inference Logs capture real-time distributions, confidence scores, and hardware metrics to drive automated promotion or fallback decisions.</p><h3 id=model-registry-object-store--metadata>Model Registry (Object Store + Metadata)</h3><table><thead><tr><th style=text-align:left>Registry Field</th><th style=text-align:left>Type</th><th style=text-align:left>Description</th><th style=text-align:left>Immutable</th></tr></thead><tbody><tr><td style=text-align:left><code>model_uri</code></td><td style=text-align:left>S3 URI</td><td style=text-align:left>Path to TorchScript/ONNX binary.</td><td style=text-align:left>Yes</td></tr><tr><td style=text-align:left><code>runtime_env</code></td><td style=text-align:left>Container Tag</td><td style=text-align:left>Python/C++ environment version.</td><td style=text-align:left>Yes</td></tr><tr><td style=text-align:left><code>is_live</code></td><td style=text-align:left>Boolean</td><td style=text-align:left>Global flag for production routing.</td><td style=text-align:left>No</td></tr><tr><td style=text-align:left><code>fallback_id</code></td><td style=text-align:left>Version ID</td><td style=text-align:left>Directs to smaller model if latency spikes.</td><td style=text-align:left>No</td></tr></tbody></table><h3 id=inference-logs-sampled--streaming>Inference Logs (Sampled / Streaming)</h3><table><thead><tr><th style=text-align:left>Dimension</th><th style=text-align:left>Description</th><th style=text-align:left>Retention</th></tr></thead><tbody><tr><td style=text-align:left><strong>Input Features</strong></td><td style=text-align:left>Vector/Tensor used for prediction.</td><td style=text-align:left>14 days</td></tr><tr><td style=text-align:left><strong>Probability</strong></td><td style=text-align:left>Softmax/Confidence score from head.</td><td style=text-align:left>14 days</td></tr><tr><td style=text-align:left><strong>Hardware Metrics</strong></td><td style=text-align:left>GPU Mem/Util during the kernel call.</td><td style=text-align:left>30 days</td></tr></tbody></table><h2 id=deep-dive--trade-offs>Deep Dive & Trade-offs</h2><h3 id=deep-dive>Deep Dive</h3><ul><li><p><strong>Serving Runtime:</strong> High-performance inference servers (Triton/TorchServe) provide unified gRPC/REST APIs across varied GPU backends.</p></li><li><p><strong>Micro-batching:</strong> Aggregating requests within a 5-10ms window amortizes kernel launch overhead, dramatically increasing GPU throughput.</p></li><li><p><strong>Resource Management:</strong> GPUs are assigned based on memory needs, using MPS for sharing small models and isolating latency-critical traffic.</p></li><li><p><strong>Canary Rollouts:</strong> Routing 5-10% of traffic to new versions enables automated, metrics-driven promotion or rollback.</p></li><li><p><strong>Fallback & Degradation:</strong> Distilled fallback models or cached predictions automatically activate during latency spikes or primary failures to strictly meet SLOs.</p></li><li><p><strong>Preprocessing:</strong> Vectorized transforms for feature normalization run inside the serving pipeline, versioned alongside the model to eliminate training-serving skew.</p></li></ul><h3 id=trade-offs>Trade-offs</h3><ul><li><p><strong>Batch Size vs. Latency:</strong> Larger batches maximize throughput but increase tail latency (P99); requires per-model tuning to balance cost against responsiveness.</p></li><li><p><strong>GPU Sharing vs. Isolation:</strong> Sharing maximizes cost-efficiency but risks noisy-neighbor effects; Isolation guarantees performance but leaves capacity idle during troughs.</p></li></ul><h2 id=operational-excellence>Operational Excellence</h2><h3 id=slis--slos>SLIs / SLOs</h3><ul><li>SLO: P99 inference latency &lt; 200 ms for all production models.</li><li>SLO: 99.9% availability of the inference API (including fallback responses).</li><li>SLIs: inference_latency_p99, inference_error_rate, model_load_time, gpu_utilization, batch_fill_ratio, canary_accuracy_delta.</li></ul><h3 id=reliability--resiliency>Reliability & Resiliency</h3><ul><li><strong>Load-Test</strong>: Validate 2x peak QPS for each new version before canary.</li><li><strong>Shadow</strong>: Run offline baseline comparisons via production traffic mirrors.</li><li><strong>Chaos</strong>: Kill GPU nodes and verify 30s fallback/reschedule timing.</li></ul></section><aside class=related><h3>Related</h3><ul><li><a href=/systology/deep-dives/ai-ml-workshop/>AI/ML Workshop</a>
<span class=related-type><small class=label-tag>tag</small></span><br><small class=muted>Practical, reproducible ML examples (PyTorch/Hugging Face/NumPy) with MPS-aware benchmarks and experiment hygiene for local hardware.</small></li><li><a href=/systology/designs/background-job-queue/>Background Job Queue for Big Tasks</a>
<span class=related-type><small class=label-tag>tag</small>
<small class=label-cat>category</small></span><br><small class=muted>Asynchronous job queue design for resource-heavy tasks (video encoding, data processing) with retries, idempotency, DLQ handling, and autoscaling.</small></li><li><a href=/systology/deep-dives/chowist/>Chowist</a>
<span class=related-type><small class=label-tag>tag</small></span><br><small class=muted>Monolithic Django app for place listings; focus on production hardening: static serving, background jobs, connection pooling, and CI/CD.</small></li><li><a href=/systology/designs/distributed-cache/>Distributed Caching Layer for VCS</a>
<span class=related-type><small class=label-tag>tag</small>
<small class=label-cat>category</small></span><br><small class=muted>Design a distributed cache to reduce I/O and speed up VCS operations by caching objects and hashes with high concurrency and low latency.</small></li></ul></aside></article></main><footer class=site-footer role=contentinfo><div class="container footer-inner"><p class=copyright>&copy; 2026 Systology. All rights reserved.</p><div class=social-links><a href=https://github.com/huangsam target=_blank rel="noopener noreferrer" aria-label=GitHub><svg width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon-github"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77 5.44 5.44.0 003.5 8.55c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg>
</a><a href=http://linkedin.com/in/sambyte target=_blank rel="noopener noreferrer" aria-label=LinkedIn><svg width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon-linkedin"><path d="M16 8a6 6 0 016 6v7h-4v-7a2 2 0 00-2-2 2 2 0 00-2 2v7h-4v-7a6 6 0 016-6z"/><rect x="2" y="9" width="4" height="12"/><circle cx="4" cy="4" r="2"/></svg></a></div></div></footer><div id=site-search-modal class=site-search-modal-container style=display:none><div class=site-search-overlay data-close-search></div><div class=site-search-modal><div class=site-search-header><div class=site-search-input-wrapper><span class=site-search-icon><svg width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="11" cy="11" r="8"/><line x1="21" y1="21" x2="16.65" y2="16.65"/></svg>
</span><input type=text class=site-search-input id=site-search-input placeholder="Search designs, principles, deep dives..." aria-label=Search autocomplete=off>
<button class=site-search-close data-close-search>&#215;</button></div></div><div class=site-search-body><div id=site-search-default class=site-search-default><div class=site-search-quick-links><a href=/systology/categories/ class=site-search-cta><span class=site-search-cta-icon>üìÇ</span>
<span class=site-search-cta-text>All Categories</span>
<span class=site-search-cta-arrow>‚Üí</span>
</a><a href=/systology/tags/ class=site-search-cta><span class=site-search-cta-icon>üè∑Ô∏è</span>
<span class=site-search-cta-text>All Tags</span>
<span class=site-search-cta-arrow>‚Üí</span></a></div></div><div id=site-search-results-list style=display:none></div></div></div></div><script>document.addEventListener("DOMContentLoaded",function(){let i=null,o=null;const t=document.getElementById("site-search-modal"),n=document.getElementById("site-search-input"),s=document.getElementById("site-search-default"),e=document.getElementById("site-search-results-list"),d=document.querySelectorAll("[data-close-search]"),h=document.querySelectorAll("[data-open-search]");if(!t)return;document.body.appendChild(t);let l=!1;function u(){if(l)return;fetch("/systology/search-index.json").then(e=>e.json()).then(e=>{i=e.documents,l=!0}).catch(e=>console.error("Failed to load search index:",e))}function r(){u(),t.style.display="flex",document.body.style.overflow="hidden",setTimeout(()=>{n.focus(),n.value.trim()?c(n.value):(s.style.display="block",e.style.display="none")},50)}function a(){t.style.display="none",document.body.style.overflow="",n.value="",e.innerHTML="",s.style.display="block",e.style.display="none"}h.forEach(e=>{e.addEventListener("click",e=>{e.preventDefault(),r()})}),d.forEach(e=>{e.addEventListener("click",a)});function c(t){if(!i)return;if(!t.trim()){s.style.display="block",e.style.display="none",e.innerHTML="";return}s.style.display="none",e.style.display="block",t=t.toLowerCase();const n=i.filter(e=>{const n=e.content.toLowerCase(),s=e.title.toLowerCase(),o=(e.tags||[]).join(" ").toLowerCase();return n.includes(t)||s.includes(t)}).slice(0,15);n.length===0?e.innerHTML='<p class="site-search-no-results">No results found.</p>':e.innerHTML=n.map(e=>`
        <a href="${e.url}" class="site-search-result">
          <div class="site-search-result-title">${e.title}</div>
          <div class="site-search-result-preview">${e.preview}</div>
          <div class="site-search-result-meta">
            <span class="site-search-badge-category">${e.category}</span>
            ${e.tags.map(e=>`<span class="site-search-badge">${e}</span>`).join("")}
          </div>
        </a>
      `).join("")}n.addEventListener("input",function(e){o&&clearTimeout(o),o=setTimeout(()=>{c(e.target.value)},300)}),document.addEventListener("keydown",function(e){(e.metaKey||e.ctrlKey)&&e.key==="k"&&(e.preventDefault(),t.style.display==="none"||t.style.display===""?r():a()),e.key==="Escape"&&t.style.display==="flex"&&a()})})</script><script>document.addEventListener("DOMContentLoaded",()=>{const t=document.querySelectorAll(".pseudocode-toggle"),n=document.querySelectorAll(".pseudocode-modal-close"),s=document.querySelectorAll(".pseudocode-modal-overlay");function o(e){const t=document.getElementById(e);if(!t)return;t.classList.add("active"),t.setAttribute("aria-hidden","false"),document.body.style.overflow="hidden";const n=t.querySelector(".pseudocode-modal-close");n&&n.focus()}function e(e){if(!e)return;e.classList.remove("active"),e.setAttribute("aria-hidden","true"),document.body.style.overflow="";const t=document.querySelector(`.pseudocode-toggle[aria-controls="${e.id}"]`);t&&t.focus()}t.forEach(e=>{e.addEventListener("click",()=>{const t=e.getAttribute("aria-controls");o(t)})}),n.forEach(t=>{t.addEventListener("click",t=>{const n=t.target.closest(".pseudocode-modal");e(n)})}),s.forEach(t=>{t.addEventListener("click",t=>{const n=t.target.closest(".pseudocode-modal");e(n)})}),document.addEventListener("keydown",t=>{if(t.key==="Escape"){const t=document.querySelector(".pseudocode-modal.active");t&&e(t)}})})</script><script src=/systology/js/mermaid.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){window.mermaid&&mermaid.initialize({startOnLoad:!0,theme:"base",securityLevel:"loose",themeVariables:{primaryColor:"#f3f4f6",primaryTextColor:"#0f1724",primaryBorderColor:"#2563eb",lineColor:"#6b7280",secondBkgColor:"#ffffff",tertiaryTextColor:"#6b7280",tertiaryColor:"#e6e9ee",noteBkgColor:"#f0f9ff",noteBorderColor:"#2563eb",fontFamily:'-apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif'},flowchart:{useMaxWidth:!0,curve:"linear"},sequence:{useMaxWidth:!0},gantt:{useWidth:0[0]}});var e=document.getElementById("theme-toggle");e&&e.addEventListener("click",function(){var e=document.documentElement.getAttribute("data-theme")==="dark";document.documentElement.setAttribute("data-theme",e?"":"dark");try{localStorage.setItem("theme",e?"light":"dark")}catch{}})})</script></body></html>