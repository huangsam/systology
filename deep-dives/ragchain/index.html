<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Ragchain - Systology</title><meta name=description content="Retrieval-augmented generation with search and ML."><link rel=canonical href=https://sambyte.net/systology/deep-dives/ragchain/><link rel=icon href=/systology/favicon.svg type=image/svg+xml><meta name=robots content="index, follow"><script>try{localStorage.getItem("theme")==="dark"&&(document.documentElement.setAttribute("data-theme","dark"),document.documentElement.style.background="#0f1724")}catch{}</script><link rel=stylesheet href="/systology/css/styles.css?v=1772222889"><link rel=stylesheet href="/systology/css/syntax.css?v=1772222889"><link rel=stylesheet href="/systology/css/search.css?v=1772222889"></head><body><header class=site-header role=banner><div class="container header-inner"><a class=brand href=/systology/ aria-label=Systology><img src=/systology/favicon.svg alt class=logo-icon> Systology</a><nav class=site-nav role=navigation aria-label="Main navigation"><button class=search-toggle data-open-search aria-label=Search>
<svg width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="11" cy="11" r="8"/><line x1="21" y1="21" x2="16.65" y2="16.65"/></svg>
</button>
<button class=theme-toggle id=theme-toggle aria-label="Toggle dark mode">
<svg class="icon-moon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg class="icon-sun" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></nav></div></header><main id=content class="container prose"><article><header><div class=title-group><h1>Ragchain</h1><a class=link-pill href=https://github.com/huangsam/ragchain target=_blank rel=noopener title="View on GitHub">GH</a></div><h2>Retrieval-augmented generation with search and ML.</h2><div class=tags><span>Tags:</span>
<a class=badge href=/tags/ml/>ml</a>
<a class=badge href=/tags/privacy/>privacy</a>
<a class=badge href=/tags/retrieval/>retrieval</a></div></header><section><h2 id=context--motivation>Context & Motivation</h2><p><strong>Context:</strong> <code>ragchain</code> is a local-first RAG (Retrieval-Augmented Generation) stack built on Chroma for vector storage and Ollama for local LLM inference. Everything runs on-device‚Äîembeddings, vector search, BM25 retrieval, and LLM generation‚Äîensuring that user documents and queries never leave local infrastructure.</p><p><strong>Motivation:</strong> I have a deep interest in asking insightful questions about programming languages from time to time. One of the websites I keep coming back to is TIOBE, which has all the numbers but not as much about the language or the underlying concepts itself. When I want to try out a new language on that list, I want a chatbot that can answer questions about the language, its history, and its ecosystem based on the TIOBE content.</p><h2 id=the-local-implementation>The Local Implementation</h2><ul><li><strong>Current Logic:</strong> Documents are ingested via a CLI (<code>ragchain ingest</code>) that chunks text with configurable size and overlap (default 2500 chars / 500 overlap), generates embeddings via Ollama&rsquo;s local embedding model (qwen3-embedding), and stores vectors in Chroma with document metadata. A parallel BM25 index (rank_bm25) is built over the same chunks for lexical retrieval. Queries are served via <code>ragchain ask</code>, which uses LangGraph to orchestrate intent-based adaptive retrieval‚Äîclassifying queries as FACT, CONCEPT, or COMPARISON and adjusting BM25/vector weights accordingly‚Äîthen combines results using RRF (score = Œ£ 1/(k + rank) across retrievers) and feeds the top-k chunks as context to Ollama for generation.</li><li><strong>Hybrid retrieval details:</strong> BM25 excels at exact keyword matches (error codes, proper nouns, specific terms) while semantic search handles paraphrases and conceptual queries. The intent router classifies queries and assigns retrieval weights: FACT queries use 0.8 BM25 / 0.2 Chroma (keyword-heavy for enumerations), CONCEPT queries use 0.4 BM25 / 0.6 Chroma (balanced), and COMPARISON queries use 0.3 BM25 / 0.7 Chroma (semantic-heavy). A document relevance grader validates retrieved results, and automatic query rewriting retries retrieval on failure (max 1 retry).</li><li><strong>Self-correcting pipeline:</strong> the LangGraph orchestrator implements a conditional retry loop‚Äîif the grader deems retrieved documents irrelevant, the query rewriter enhances the query and re-retrieves. This self-correcting behavior significantly improves answer quality for ambiguous or poorly-phrased queries.</li><li><strong>Privacy architecture:</strong> no data leaves the device. Ollama runs models locally (currently qwen3 for generation, qwen3-embedding for embeddings), Chroma stores vectors on local disk or via a local Docker container, and the BM25 index is an in-memory structure rebuilt from local documents.</li><li><strong>Bottleneck:</strong> Embedding generation is the ingestion bottleneck‚ÄîOllama on CPU can be slow per chunk, making large re-indexing runs time-consuming. Memory footprint grows with corpus size (Chroma vectors + BM25 term frequencies). The default corpus covers 50 programming languages plus 10 conceptual bridge pages, and extending to larger domains requires attention to chunk sizing and retrieval parameter tuning.</li></ul><h2 id=comparison-to-industry-standards>Comparison to Industry Standards</h2><ul><li><strong>My Project:</strong> Local-only RAG with explicit reproducibility and privacy guarantees. No cloud dependencies. Hybrid retrieval with systematic evaluation. Suitable for personal knowledge bases and privacy-sensitive domains.</li><li><strong>Industry:</strong> Cloud RAG offerings (Pinecone + OpenAI, Weaviate Cloud, Amazon Kendra) provide managed scaling, automatic sharding, and access to powerful embedding/generation models, but trade off privacy, repeatability, and cost transparency. They also abstract away retrieval tuning, making quality debugging opaque.</li><li><strong>Gap Analysis:</strong> To reach production-grade scale and latency: implement persistent sharding with cross-shard query routing, add ANN (HNSW) search for indices beyond 1M vectors (Chroma uses exact search by default), integrate monitoring for retrieval quality drift (track average relevance scores over time), and build a reranker layer (cross-encoder on top-50 results) to recover precision lost by approximate search.</li></ul><h2 id=risks--mitigations>Risks & Mitigations</h2><ul><li><strong>Stale embeddings:</strong> incremental re-embedding on document change detection (content hash comparison). Schedule periodic full re-index validation that compares incremental vs. clean-build index quality to detect accumulated drift.</li><li><strong>Resource exhaustion:</strong> provide configurable limits on corpus size and in-memory BM25 index size. Guide users to shard or prune indices when memory exceeds thresholds. Document minimum hardware requirements per corpus size.</li><li><strong>Embedding model drift:</strong> pin embedding model versions in the index manifest. When upgrading models, re-embed the entire corpus atomically and compare retrieval quality before and after using the evaluation suite.</li><li><strong>LLM hallucination:</strong> log the retrieved context alongside generated answers for auditability. Surface retrieval confidence scores to the user so low-confidence answers are flagged rather than presented as authoritative.</li></ul></section><aside class=related><h3>Related</h3><ul><li><a href=/systology/deep-dives/ai-ml-workshop/>AI/ML Workshop</a>
<span class=related-type><small class=label-tag>tag</small>
<small class=label-cat>category</small></span><br><small class=muted>Practical, reproducible ML examples (PyTorch/Hugging Face/NumPy) with MPS-aware benchmarks and experiment hygiene for local hardware.</small></li><li><a href=/systology/designs/feature-etl/>ETL Pipeline for ML Features</a>
<span class=related-type><small class=label-tag>tag</small></span><br><small class=muted>Robust ETL pipeline for deterministic, reproducible ML feature generation from diverse sources, with idempotence and scale in mind.</small></li><li><a href=/systology/deep-dives/mailprune/>Mailprune</a>
<span class=related-type><small class=label-tag>tag</small>
<small class=label-cat>category</small></span><br><small class=muted>Local-first email auditing and cleanup tool that identifies noisy senders and produces privacy-preserving recommendations.</small></li><li><a href=/systology/principles/ml-experiments/>ML Experiments</a>
<span class=related-type><small class=label-tag>tag</small></span><br><small class=muted>Practical guidance for reproducible, resource-aware ML experiments with lightweight MLOps and deterministic evaluation.</small></li></ul></aside></article></main><footer class=site-footer role=contentinfo><div class="container footer-inner"><p class=copyright>&copy; 2026 Systology. All rights reserved.</p><div class=social-links><a href=https://github.com/huangsam target=_blank rel="noopener noreferrer" aria-label=GitHub><svg width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon-github"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77 5.44 5.44.0 003.5 8.55c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg>
</a><a href=http://linkedin.com/in/sambyte target=_blank rel="noopener noreferrer" aria-label=LinkedIn><svg width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon-linkedin"><path d="M16 8a6 6 0 016 6v7h-4v-7a2 2 0 00-2-2 2 2 0 00-2 2v7h-4v-7a6 6 0 016-6z"/><rect x="2" y="9" width="4" height="12"/><circle cx="4" cy="4" r="2"/></svg></a></div></div></footer><div id=site-search-modal class=site-search-modal-container style=display:none><div class=site-search-overlay data-close-search></div><div class=site-search-modal><div class=site-search-header><div class=site-search-input-wrapper><span class=site-search-icon><svg width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="11" cy="11" r="8"/><line x1="21" y1="21" x2="16.65" y2="16.65"/></svg>
</span><input type=text class=site-search-input id=site-search-input placeholder="Search designs, principles, deep dives..." aria-label=Search autocomplete=off>
<button class=site-search-close data-close-search>&#215;</button></div></div><div class=site-search-body><div id=site-search-default class=site-search-default><div class=site-search-quick-links><a href=/systology/categories/ class=site-search-cta><span class=site-search-cta-icon>üìÇ</span>
<span class=site-search-cta-text>All Categories</span>
<span class=site-search-cta-arrow>‚Üí</span>
</a><a href=/systology/tags/ class=site-search-cta><span class=site-search-cta-icon>üè∑Ô∏è</span>
<span class=site-search-cta-text>All Tags</span>
<span class=site-search-cta-arrow>‚Üí</span></a></div></div><div id=site-search-results-list style=display:none></div></div></div></div><script>document.addEventListener("DOMContentLoaded",function(){let i=null,o=null;const t=document.getElementById("site-search-modal"),n=document.getElementById("site-search-input"),s=document.getElementById("site-search-default"),e=document.getElementById("site-search-results-list"),d=document.querySelectorAll("[data-close-search]"),h=document.querySelectorAll("[data-open-search]");if(!t)return;document.body.appendChild(t);let l=!1;function u(){if(l)return;fetch("/systology/search-index.json").then(e=>e.json()).then(e=>{i=e.documents,l=!0}).catch(e=>console.error("Failed to load search index:",e))}function r(){u(),t.style.display="flex",document.body.style.overflow="hidden",setTimeout(()=>{n.focus(),n.value.trim()?c(n.value):(s.style.display="block",e.style.display="none")},50)}function a(){t.style.display="none",document.body.style.overflow="",n.value="",e.innerHTML="",s.style.display="block",e.style.display="none"}h.forEach(e=>{e.addEventListener("click",e=>{e.preventDefault(),r()})}),d.forEach(e=>{e.addEventListener("click",a)});function c(t){if(!i)return;if(!t.trim()){s.style.display="block",e.style.display="none",e.innerHTML="";return}s.style.display="none",e.style.display="block",t=t.toLowerCase();const n=i.filter(e=>{const n=e.content.toLowerCase(),s=e.title.toLowerCase(),o=(e.tags||[]).join(" ").toLowerCase();return n.includes(t)||s.includes(t)}).slice(0,15);n.length===0?e.innerHTML='<p class="site-search-no-results">No results found.</p>':e.innerHTML=n.map(e=>`
        <a href="${e.url}" class="site-search-result">
          <div class="site-search-result-title">${e.title}</div>
          <div class="site-search-result-preview">${e.preview}</div>
          <div class="site-search-result-meta">
            <span class="site-search-badge-category">${e.category}</span>
            ${e.tags.map(e=>`<span class="site-search-badge">${e}</span>`).join("")}
          </div>
        </a>
      `).join("")}n.addEventListener("input",function(e){o&&clearTimeout(o),o=setTimeout(()=>{c(e.target.value)},300)}),document.addEventListener("keydown",function(e){(e.metaKey||e.ctrlKey)&&e.key==="k"&&(e.preventDefault(),t.style.display==="none"||t.style.display===""?r():a()),e.key==="Escape"&&t.style.display==="flex"&&a()})})</script><script>document.addEventListener("DOMContentLoaded",()=>{const t=document.querySelectorAll(".pseudocode-toggle"),n=document.querySelectorAll(".pseudocode-modal-close"),s=document.querySelectorAll(".pseudocode-modal-overlay");function o(e){const t=document.getElementById(e);if(!t)return;t.classList.add("active"),t.setAttribute("aria-hidden","false"),document.body.style.overflow="hidden";const n=t.querySelector(".pseudocode-modal-close");n&&n.focus()}function e(e){if(!e)return;e.classList.remove("active"),e.setAttribute("aria-hidden","true"),document.body.style.overflow="";const t=document.querySelector(`.pseudocode-toggle[aria-controls="${e.id}"]`);t&&t.focus()}t.forEach(e=>{e.addEventListener("click",()=>{const t=e.getAttribute("aria-controls");o(t)})}),n.forEach(t=>{t.addEventListener("click",t=>{const n=t.target.closest(".pseudocode-modal");e(n)})}),s.forEach(t=>{t.addEventListener("click",t=>{const n=t.target.closest(".pseudocode-modal");e(n)})}),document.addEventListener("keydown",t=>{if(t.key==="Escape"){const t=document.querySelector(".pseudocode-modal.active");t&&e(t)}})})</script><script src=/systology/js/mermaid.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){window.mermaid&&mermaid.initialize({startOnLoad:!0,theme:"base",securityLevel:"loose",themeVariables:{primaryColor:"#f3f4f6",primaryTextColor:"#0f1724",primaryBorderColor:"#2563eb",lineColor:"#6b7280",secondBkgColor:"#ffffff",tertiaryTextColor:"#6b7280",tertiaryColor:"#e6e9ee",noteBkgColor:"#f0f9ff",noteBorderColor:"#2563eb",fontFamily:'-apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif'},flowchart:{useMaxWidth:!0,curve:"linear"},sequence:{useMaxWidth:!0},gantt:{useWidth:0[0]}});var e=document.getElementById("theme-toggle");e&&e.addEventListener("click",function(){var e=document.documentElement.getAttribute("data-theme")==="dark";document.documentElement.setAttribute("data-theme",e?"":"dark");try{localStorage.setItem("theme",e?"light":"dark")}catch{}})})</script></body></html>